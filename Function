from pyspark.sql import functions as sf
from pyspark.sql import DataFrame

def confusion_stats(stats: DataFrame, grouping: list) -> DataFrame:
    return (
        stats.groupBy(*grouping)
        .pivot("confusion_matrix_label", ["TP", "FP", "FN", "TN", "TP2"])
        .count()
        .withColumn("TP", sf.coalesce(sf.col("TP"), sf.lit(0)))
        .withColumn("TP2", sf.coalesce(sf.col("TP2"), sf.lit(0)))
        .withColumn("FP", sf.coalesce(sf.col("FP"), sf.lit(0)))
        .withColumn("FN", sf.coalesce(sf.col("FN"), sf.lit(0)))
        .withColumn("TN", sf.coalesce(sf.col("TN"), sf.lit(0)))
        .withColumn(
            "accuracy",
            sf.round(
                sf.expr("try_divide((TP + TN), (TP + TN + FP + FN))"),
                2
            ),
        )
        .withColumn(
            "precision",
            sf.round(
                sf.expr("try_divide(TP, (TP + FP))"),
                2
            ),
        )
        .withColumn(
            "recall",
            sf.round(
                sf.expr("try_divide(TP, (TP + FN))"),
                2
            ),
        )
        .withColumn(
            "f1",
            sf.round(
                sf.expr("try_divide(2 * (recall * precision), (recall + precision))"),
                2
            ),
        )
    )
